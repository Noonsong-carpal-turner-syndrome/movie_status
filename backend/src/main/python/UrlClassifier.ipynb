{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt  # tokenizing(url, title)\n",
    "import nltk # vectorizing(X_token)\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''불용어 제거 및 토큰화'''\n",
    "\n",
    "def tokenizing(url,title):\n",
    "    kr_norm = []\n",
    "    eng_norm = []\n",
    "    try:\n",
    "        kr_tokens = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣]+\", \" \", title.lower())\n",
    "        eng_tokens = re.sub(\"[^A-Za-z]+\", \" \", title.lower()) + re.sub(\"[^A-Za-z]+\", \" \", url.lower())\n",
    "    except Exception as e: pass\n",
    "    kr_norm.append(kr_tokens)\n",
    "    eng_norm.append(eng_tokens)\n",
    "    kr_stopwords=['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "    eng_stopwords=['https','http','www','com','co','kr','org','ac'] #불용어 제거하기\n",
    "    \n",
    "    okt = Okt()\n",
    "    X_token=[]\n",
    "    for sentence in kr_norm:\n",
    "        temp_X = []\n",
    "        temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in kr_stopwords] # 불용어 제거\n",
    "        X_token.append(temp_X)\n",
    "\n",
    "    ps=LancasterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemData=[]\n",
    "    for sentence in eng_norm:\n",
    "        tokenData = nltk.word_tokenize(sentence)\n",
    "        tempData = []\n",
    "        for word in tokenData:  # 불용어 제거\n",
    "            if word not in stop_words and word not in eng_stopwords:\n",
    "                word = ps.stem(word)\n",
    "                if len(word)>1:\n",
    "                    tempData.append(word)\n",
    "        stemData.append(tempData)\n",
    "    temp = []\n",
    "    for n,m in zip(X_token,stemData):\n",
    "        temp.append(n+m)\n",
    "    X_token = temp\n",
    "    return X_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''정수 인코딩 수행'''\n",
    "\n",
    "def vectorizing(X_token):\n",
    "    max_words = 35000\n",
    "    tokenizer = Tokenizer(num_words = max_words) # 상위 35,000개의 단어만 보존\n",
    "    tokenizer.fit_on_texts(X_token) \n",
    "    X_token = tokenizer.texts_to_sequences(X_token)\n",
    "    word_to_index = tokenizer.word_index\n",
    "    max_len = max(len(l) for l in X_token)\n",
    "    X_data = pad_sequences(X_token, maxlen=max_len)\n",
    "    return X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicting(X_data):\n",
    "    model = load_model('model1.h5')\n",
    "    predict = model.predict_classes(X_data)\n",
    "    # 기존 X_data, y_data json 파일에 X_data, predict 추가하기=> training(url)에 필요\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifying(url, predict):\n",
    "    # mongodb에서 같은 domain인 url들 검색해서 label 갖고오기 > 평균내기\n",
    "    domain = url.split('/').str[2]\n",
    "    conn = MongoClient('mongodb+srv://youngbeen:sm1613362@chrome-screentime.vmdiu.mongodb.net/myFirstDatabase?retryWrites=true&w=majority')\n",
    "    db = conn.chrome-screentime\n",
    "    collection = db.urls\n",
    "    results = collection.find({\"domain\":domain},{\"_id\":false,\"url\":false,\"label\":true,\"domain\":false})\n",
    "    client.close()\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''prediction 결과를 바탕으로 추가 training(미완-부가 기능)'''\n",
    "def training(url):\n",
    "    max_words = 35000\n",
    "    # X_data, y_data json으로 저장해놓고 불러오기\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size= 0.3, random_state=1234)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_words, 100))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(7, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split = 0.2)\n",
    "    model.save('model1.h5')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request\n",
    "from flask_restx import Api, Resource\n",
    "from flask import send_file, url_for\n",
    "import pickle\n",
    "import sqlite3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)  # Flask 객체 선언, 파라미터로 어플리케이션 패키지의 이름을 넣어줌.\r\n",
    "api = Api(app)  # Flask 객체에 Api 객체 등록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@api.route('/model/classify')\n",
    "class Classifier(Resource):\n",
    "    def get(self):\n",
    "        # url, title 받아오기\n",
    "        return {'result': name}, 200\n",
    "    def post(self):\n",
    "        data = request.json.get()\n",
    "        return # label 보내주기"
   ]
  },
  {
   "source": [
    "@api.route('/model/train', methods = ['POST', 'GET'])\n",
    "class Trainer(Resource):\n",
    "    def post():\n",
    "        \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(debug = True)"
   ]
  }
 ]
}