{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd077160980b57d06325a3d3988f966ff7044cd2b89b77b17d41e2f6eb795137afc",
   "display_name": "Python 3.7.10 64-bit ('capstone': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt  # tokenizing(url, title)\n",
    "import nltk # vectorizing(X_token)\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''불용어 제거 및 토큰화'''\n",
    "\n",
    "def tokenizing(url,title):\n",
    "    kr_norm = []\n",
    "    eng_norm = []\n",
    "    try:\n",
    "        kr_tokens = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣]+\", \" \", title.lower())\n",
    "        eng_tokens = re.sub(\"[^A-Za-z]+\", \" \", title.lower()) + re.sub(\"[^A-Za-z]+\", \" \", url.lower())\n",
    "    except Exception as e: pass\n",
    "    kr_norm.append(kr_tokens)\n",
    "    eng_norm.append(eng_tokens)\n",
    "    kr_stopwords=['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "    eng_stopwords=['https','http','www','com','co','kr','org','ac'] #불용어 제거하기\n",
    "    \n",
    "    okt = Okt()\n",
    "    X_token=[]\n",
    "    for sentence in kr_norm:\n",
    "        temp_X = []\n",
    "        temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in kr_stopwords] # 불용어 제거\n",
    "        X_token.append(temp_X)\n",
    "\n",
    "    ps=LancasterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemData=[]\n",
    "    for sentence in eng_norm:\n",
    "        tokenData = nltk.word_tokenize(sentence)\n",
    "        tempData = []\n",
    "        for word in tokenData:  # 불용어 제거\n",
    "            if word not in stop_words and word not in eng_stopwords:\n",
    "                word = ps.stem(word)\n",
    "                if len(word)>1:\n",
    "                    tempData.append(word)\n",
    "        stemData.append(tempData)\n",
    "    temp = []\n",
    "    for n,m in zip(X_token,stemData):\n",
    "        temp.append(n+m)\n",
    "    X_token = temp\n",
    "    return X_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''정수 인코딩 수행'''\n",
    "\n",
    "def vectorizing(X_token):\n",
    "    max_words = 35000\n",
    "    tokenizer = Tokenizer(num_words = max_words) # 상위 35,000개의 단어만 보존\n",
    "    tokenizer.fit_on_texts(X_token) \n",
    "    X_token = tokenizer.texts_to_sequences(X_token)\n",
    "    word_to_index = tokenizer.word_index\n",
    "    max_len = max(len(l) for l in X_token)\n",
    "    X_data = pad_sequences(X_token, maxlen=max_len)\n",
    "    return X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicting(X_data):\n",
    "    model = load_model('model1.h5')\n",
    "    predict = model.predict_classes(X_data)\n",
    "    # 기존 X_data, y_data json 파일에 X_data, predict 추가하기=> training(url)에 필요\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''url과 같은 domain의 최빈 label 리턴'''\n",
    "\n",
    "def classifying(url):\n",
    "    # mongodb에서 같은 domain인 url들 검색해서 label 갖고오기 > 최빈값\n",
    "    domain = url.split('/').str[2]\n",
    "    conn = MongoClient('mongodb+srv://youngbeen:sm1613362@chrome-screentime.vmdiu.mongodb.net/myFirstDatabase?retryWrites=true&w=majority')\n",
    "    db = conn.chrome-screentime\n",
    "    collection = db.urls\n",
    "    documents = collection.find({\"domain\":domain},{\"_id\":false,\"url\":false,\"label\":true,\"domain\":false})\n",
    "    client.close()\n",
    "    \n",
    "    labels =[]\n",
    "    for doc in documents:\n",
    "        labels.append(doc['label'])\n",
    "    cnt = Counter(labels)\n",
    "    result = cnt.most_common(1)[0][0]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''prediction 결과를 바탕으로 추가 training(미완-부가 기능)'''\n",
    "def training(url):\n",
    "    max_words = 35000\n",
    "    # X_data, y_data json으로 저장해놓고 불러오기\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size= 0.3, random_state=1234)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_words, 100))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(7, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split = 0.2)\n",
    "    model.save('model1.h5')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request\n",
    "from flask_restx import Api, Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)  # Flask 객체 선언, 파라미터로 어플리케이션 패키지의 이름을 넣어줌.\r\n",
    "api = Api(app)  # Flask 객체에 Api 객체 등록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@api.route('/classify')\n",
    "class Classifier(Resource):\n",
    "    def post(self):\n",
    "        data = request.get_json()\n",
    "        url = str(data['url'])\n",
    "        title = str(data['title'])\n",
    "        domain = url.split('/').str[2]\n",
    "        X_token = tokenizing(url,title)\n",
    "        X_data = vectorizing(X_token)\n",
    "\n",
    "        predicted_url = {\n",
    "            \"url\" : url,\n",
    "            \"domain\" : domain,\n",
    "            \"label\" : predicting(X_data)\n",
    "        }\n",
    "        conn = MongoClient('mongodb+srv://youngbeen:sm1613362@chrome-screentime.vmdiu.mongodb.net/myFirstDatabase?retryWrites=true&w=majority')\n",
    "        db = conn.chrome-sdreentime\n",
    "        collection = db.urls\n",
    "        documents = collection.insert(predicted_url)\n",
    "        client.close()\n",
    "\n",
    "        label = classifying(url)\n",
    "        return label # label 보내주기"
   ]
  },
  {
   "source": [
    "@api.route('/model/train', methods = ['POST', 'GET'])\n",
    "class Trainer(Resource):\n",
    "    def post():\n",
    "        \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "SystemExit",
     "evalue": "2",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(debug = True, host=\"127.0.0.1\", port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}